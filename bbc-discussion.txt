(a)
According to the class distribution generated by step(2), the classes are mostly evenly
distributed. In this task, all classes are equally important. When all classes are equally
important and equally represented, accuracy is the best metric to use to evaluate the performance.
Because it cares about the portion of instances in the entire test set that the model
correctly classifies regardless of the classes. However, precision cares about the proportion of
instances within the class of interest are actually correct and recall cares about the instances
in the class of interest are labelled correctly. These are better used when some classes
are more important than the others. Therefore, precision is the best metric for this dataset.

(b)
The performance of step 8 is identical to performance of step 7.
This is because the two models runs with the exactly same parameter and same dataset.
Essentially when the model is trained with the same dataset with same hyperparameters,
the prior probability for each class and the conditional probability for each feature
will turn out to be exactly the same. As a result, when testing on the same test set,
the predicted results are expected to be identical.

Then in step 9 and 10 where the smoothing values are changed, the reported data are slightly
different from that of step 7.

The accuracy, macro F1 and weighted F1 for step 10 (smoothing = 0.9) and 7 turns out to have no obvious difference
(maybe there's difference beyond the scale of 10e-6, but it's beyond the scope of our observation already).
This is because the default smoothing value for this model is 1.0, and we only changed the smoothing
by 0.1, which will only slightly affect the log-prob of each feature. The little changes in the log-prob
of each feature is not great enough to affect the classifications.
As discussed above, the log-prob of each feature is slightly changed, and this is indicated
by part(k) of the report where the log-prob of the favorite word in step 10 is smaller by about 0.1
than those in step 7.

The accuracy, macro F1 and weighted F1 in step 9 (smoothing = 0.0001) turns out to be
about 0.05 higher than step 10 and 7. This is because of the change of smoothing value from
1 to 0.0001 that decreases the log-prob of each feature. With these changes being applied,
the log-prob of those words that does not appear in one article will be a lot lower, which
actually helps eliminate some confusions, i.e. the classifier might wrongly classify a passage
because it was confused by the words that never appear in the class but still had some probability
due to the smoothing.
And as expected, the log-prob of the favorite word in step 9 is smaller by about 2.5, because
of the change of smoothing value. 